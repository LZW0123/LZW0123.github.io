<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AC、A2C、A3C算法简易区别</title>
    <url>/2022/04/16/AC%E3%80%81A2C%E3%80%81A3C%E7%AE%97%E6%B3%95%E7%AE%80%E6%98%93%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<ul>
<li><a href="https://zhuanlan.zhihu.com/p/62100741">参考原文</a> # AC算法</li>
</ul>
<ol type="1">
<li>策略梯度如下式：<img src="https://cdn.jsdelivr.net/gh/LZW0123/PictureBed/img/20220406095858.png" />
<ul>
<li>其中, <span class="math inline">\(\pi_{\theta}(a \mid s)\)</span> 为Actor， <span class="math inline">\(\Psi_{t}\)</span> 称为Critic，此式是一个广义的AC框架。</li>
</ul></li>
<li><span class="math inline">\(\Psi_{t}\)</span> 可以取以下几种：
<ul>
<li>轨迹总回报，<span class="math inline">\(\Sigma_{t=0}^{\infty} r_{t}\)</span></li>
<li>执行动作后的回报，<span class="math inline">\(\Sigma_{t^{\prime}=t}^{\infty} \boldsymbol{r}_{t^{\prime}}\)</span></li>
<li>加入基线的形式，<span class="math inline">\(\sum_{t^{\prime}=t}^{\infty} r_{t^{\prime}}-b\left(s_{t}\right)\)</span></li>
<li>状态-行为值函数，<span class="math inline">\(Q^{\pi}\left(s_{t}, a_{t}\right)\)</span></li>
<li>优势函数，<span class="math inline">\(A^{\pi}\left(s_{t}, a_{t}\right)\)</span></li>
<li>TD-error，<span class="math inline">\(r_{t}+V^{\pi}\left(s_{t+1}\right)-V^{\pi}\left(s_{t}\right)\)</span></li>
</ul></li>
<li>前三个critic直接利用轨迹的累积回报，由此计算出来的策略不存在偏差，但是由于是多步的累积回报，因此方差很大</li>
<li>后三个利用动作值函数，优势函数和TD偏差来代替累积回报，因而方差下，但是由于这三种方法都用到了逼近，因此计算出来的策略梯度存在偏差。当critic取后三个时，为经典的AC算法。</li>
<li><img src="https://cdn.jsdelivr.net/gh/LZW0123/PictureBed/img/20220406203012.png" /></li>
</ol>
<h1 id="a2c算法">A2C算法</h1>
<ol type="1">
<li>A2C使用优势函数代替Critic网络中的原始回报，可以作为衡量选取动作值和所有动作平均值好坏的指标。</li>
<li>优势函数：<img src="https://cdn.jsdelivr.net/gh/LZW0123/PictureBed/img/20220406203607.png" /></li>
<li>意义：如果优势函数大于0，则说明该动作比平均动作好，如果优势函数小于0，则说明当前动作还不如平均动作好
<ul>
<li>理解：如果优势函数大于0 ，那么Q&gt;V，V在里面可以表示一个平均动作下的价值</li>
</ul></li>
</ol>
<h1 id="a3c算法">A3C算法</h1>
<ol type="1">
<li>异步优势动作评价算法，存在多个并行环境来收集数据，打破数据之间的关联性。</li>
</ol>
]]></content>
      <tags>
        <tag>RL</tag>
      </tags>
  </entry>
  <entry>
    <title>test</title>
    <url>/2022/04/15/test/</url>
    <content><![CDATA[<p>这是我的第一个博客！</p>
]]></content>
  </entry>
  <entry>
    <title>RL机器人控制面临的问题</title>
    <url>/2022/05/04/RL%E6%9C%BA%E5%99%A8%E4%BA%BA%E6%8E%A7%E5%88%B6%E9%9D%A2%E4%B8%B4%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<ol type="1">
<li>RL机器人问题，是连续高维动作和状态空间</li>
<li>RL机器人控制三个方面问题：
<ul>
<li>数据效率</li>
<li>探索与利用</li>
<li>泛化性和可复现性</li>
</ul></li>
<li>数据效率低
<ul>
<li>提高数据效率的方法之一是收集更多的数据和更有效地使用目前拥有的数据</li>
<li>收集更多数据的方法之一是并行地运行多个机器人来收集数据</li>
</ul></li>
<li>探索与利用
<ul>
<li>真实机器人进行探索，可能会损伤机器人</li>
<li>同策略方法中探索性取决于初始的条件和训练过程。在训练策略的过程，可能会注重于利用</li>
</ul></li>
<li>泛化性和可复现性
<ul>
<li>当前的某些算法只注重于某一种任务，而到了另一种任务的时候需要重新调节参数</li>
<li>随机种子数影响着能否复现成功</li>
</ul></li>
</ol>
]]></content>
      <tags>
        <tag>RL</tag>
        <tag>机器人控制</tag>
      </tags>
  </entry>
</search>
